# NOAAå¤©æ°”æ•°æ®è·å– - å¿«é€Ÿå…¥é—¨æŒ‡å—

## ğŸ“‹ ç›®æ ‡
ä¸ºä½ çš„2018-2022èˆªç­å»¶è¯¯é¢„æµ‹é¡¹ç›®è·å–å¤©æ°”æ•°æ®

## ğŸ¯ æ–¹æ¡ˆæ¦‚è¿°
ä½¿ç”¨ **meteostat** Pythonåº“è·å–å†å²å¤©æ°”æ•°æ®ï¼ˆæœ€ç®€å•æ¨èçš„æ–¹æ³•ï¼‰

---

## ç¬¬ä¸€æ­¥ï¼šå®‰è£…ä¾èµ– (5åˆ†é’Ÿ)

```bash
# å®‰è£…å¿…è¦çš„Pythonåº“
pip install meteostat pandas pyarrow requests

# éªŒè¯å®‰è£…
python -c "import meteostat; print('meteostatå®‰è£…æˆåŠŸ')"
```

---

## ç¬¬äºŒæ­¥ï¼šå¿«é€Ÿæµ‹è¯• (5åˆ†é’Ÿ)

è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯ä¸€åˆ‡æ­£å¸¸ï¼š

```bash
python test_weather_download.py
```

**é¢„æœŸè¾“å‡º**ï¼š
- âœ“ æ‰¾åˆ°JFKé™„è¿‘çš„æ°”è±¡ç«™
- âœ“ ä¸‹è½½2018å¹´1æœˆä¸€å‘¨çš„æ•°æ®
- âœ“ ç”Ÿæˆ `jfk_weather_sample.csv` æ–‡ä»¶

å¦‚æœçœ‹åˆ° âœ“ æµ‹è¯•æˆåŠŸï¼Œè¯´æ˜ç¯å¢ƒé…ç½®æ­£ç¡®ï¼

---

## ç¬¬ä¸‰æ­¥ï¼šå‡†å¤‡æœºåœºåˆ—è¡¨ (15åˆ†é’Ÿ)

### æ–¹æ³•Aï¼šä½¿ç”¨æä¾›çš„æœºåœºåˆ—è¡¨
```bash
python create_airport_mapping.py
```
è¿™ä¼šç”Ÿæˆ `us_airports_coordinates.csv`ï¼ŒåŒ…å«ç¾å›½æœ€ç¹å¿™çš„100ä¸ªæœºåœºã€‚

### æ–¹æ³•Bï¼šä»ä½ çš„èˆªç­æ•°æ®æå–
```python
import pandas as pd
from pyspark.sql import SparkSession

# å¯åŠ¨Spark
spark = SparkSession.builder.appName("ExtractAirports").getOrCreate()

# è¯»å–ä½ çš„èˆªç­æ•°æ®
flight_df = spark.read.csv('your_flight_data.csv', header=True)

# æå–æ‰€æœ‰å”¯ä¸€æœºåœº
origins = flight_df.select('Origin').distinct()
dests = flight_df.select('Dest').distinct()

# åˆå¹¶å¹¶å»é‡
all_airports = origins.union(dests).distinct()
all_airports.write.csv('my_airports.csv', header=True)
```

---

## ç¬¬å››æ­¥ï¼šæ‰¹é‡ä¸‹è½½å¤©æ°”æ•°æ® (2-4å°æ—¶)

### å®Œæ•´å·¥ä½œæµè„šæœ¬

```python
import pandas as pd
from meteostat import Stations, Hourly
from datetime import datetime
import time

# 1. è¯»å–æœºåœºåˆ—è¡¨
airports_df = pd.read_csv('us_airports_coordinates.csv')

# 2. ä¸ºæ¯ä¸ªæœºåœºæ‰¾æ°”è±¡ç«™
def find_weather_station(lat, lon):
    stations = Stations()
    stations = stations.nearby(lat, lon)
    station = stations.fetch(1)
    return station.index[0] if not station.empty else None

airports_df['station_id'] = airports_df.apply(
    lambda row: find_weather_station(row['latitude'], row['longitude']), 
    axis=1
)

print(f"æ˜ å°„å®Œæˆï¼Œ{airports_df['station_id'].notna().sum()}/{len(airports_df)} ä¸ªæœºåœºæ‰¾åˆ°æ°”è±¡ç«™")

# 3. æ‰¹é‡ä¸‹è½½å¤©æ°”æ•°æ®
all_weather = []

for idx, row in airports_df.iterrows():
    if pd.isna(row['station_id']):
        continue
    
    print(f"\nå¤„ç† {row['airport_code']} ({idx+1}/{len(airports_df)})...")
    
    for year in [2018, 2019, 2020, 2021, 2022]:
        try:
            start = datetime(year, 1, 1)
            end = datetime(year, 12, 31, 23)
            
            data = Hourly(row['station_id'], start, end)
            df = data.fetch()
            
            if not df.empty:
                df['airport_code'] = row['airport_code']
                df.reset_index(inplace=True)
                all_weather.append(df)
                print(f"  âœ“ {year}: {len(df)} æ¡è®°å½•")
            else:
                print(f"  âœ— {year}: æ— æ•°æ®")
        except Exception as e:
            print(f"  âœ— {year}: {e}")
        
        time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

# 4. åˆå¹¶å¹¶ä¿å­˜
if all_weather:
    combined = pd.concat(all_weather, ignore_index=True)
    
    # ä¿å­˜ä¸ºParquetï¼ˆPySparkå‹å¥½ä¸”å‹ç¼©ç‡é«˜ï¼‰
    combined.to_parquet('weather_data_2018_2022.parquet', index=False)
    
    print(f"\nâœ“ å®Œæˆï¼æ€»å…± {len(combined)} æ¡è®°å½•")
    print(f"âœ“ æ–‡ä»¶å¤§å°: {combined.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
    print(f"âœ“ ä¿å­˜ä¸º: weather_data_2018_2022.parquet")
else:
    print("\nâœ— æœªè·å–åˆ°ä»»ä½•æ•°æ®")
```

### ä¿å­˜å¹¶è¿è¡Œ
```bash
# å°†ä¸Šé¢çš„ä»£ç ä¿å­˜ä¸º download_all_weather.py
python download_all_weather.py
```

**é¢„è®¡æ—¶é—´**ï¼š
- 100ä¸ªæœºåœº Ã— 5å¹´ = çº¦2-4å°æ—¶
- å¯ä»¥ä¸­æ–­åç»§ç»­è¿è¡Œï¼ˆå·²ä¸‹è½½çš„æ•°æ®ä¸ä¼šé‡å¤ï¼‰

---

## ç¬¬äº”æ­¥ï¼šæ•°æ®è´¨é‡æ£€æŸ¥

```python
import pandas as pd

# è¯»å–æ•°æ®
df = pd.read_parquet('weather_data_2018_2022.parquet')

print("æ•°æ®æ¦‚è§ˆï¼š")
print(f"æ€»è®°å½•æ•°: {len(df):,}")
print(f"æ—¶é—´è·¨åº¦: {df['time'].min()} åˆ° {df['time'].max()}")
print(f"è¦†ç›–æœºåœº: {df['airport_code'].nunique()} ä¸ª")

print("\nå­—æ®µåˆ—è¡¨ï¼š")
print(df.columns.tolist())

print("\nç¼ºå¤±å€¼ç»Ÿè®¡ï¼š")
print(df.isnull().sum())

print("\nå„æœºåœºè®°å½•æ•°ï¼š")
print(df['airport_code'].value_counts().head(10))
```

---

## ç¬¬å…­æ­¥ï¼šä¸èˆªç­æ•°æ®åˆå¹¶ (PySpark)

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, hour, to_timestamp, broadcast

spark = SparkSession.builder \
    .appName("MergeWeatherData") \
    .config("spark.driver.memory", "8g") \
    .getOrCreate()

# 1. è¯»å–èˆªç­æ•°æ®
flight_df = spark.read.csv('your_flight_data.csv', header=True)

# 2. è¯»å–å¤©æ°”æ•°æ®
weather_df = spark.read.parquet('weather_data_2018_2022.parquet')

# 3. å‡†å¤‡æ—¶é—´å­—æ®µ
# å‡è®¾èˆªç­æ•°æ®æœ‰ FlightDate å’Œ DepTime
flight_df = flight_df.withColumn(
    'flight_datetime',
    to_timestamp(concat(col('FlightDate'), col('DepTime')), 'yyyyMMddHHmm')
)

weather_df = weather_df.withColumn(
    'weather_hour',
    to_timestamp(col('time'))
)

# 4. åˆå¹¶èµ·é£æœºåœºå¤©æ°”ï¼ˆä½¿ç”¨12å°æ—¶å‰çš„å¤©æ°”ä½œä¸º"é¢„æµ‹"ç‰¹å¾ï¼‰
flight_with_weather = flight_df.join(
    broadcast(weather_df.select(
        col('airport_code').alias('weather_airport'),
        col('weather_hour'),
        col('temp').alias('origin_temp_12h_ago'),
        col('wspd').alias('origin_wind_12h_ago'),
        col('prcp').alias('origin_precip_12h_ago'),
        # æ·»åŠ å…¶ä»–éœ€è¦çš„å¤©æ°”å­—æ®µ
    )),
    (col('Origin') == col('weather_airport')) &
    (col('weather_hour') == col('flight_datetime') - expr('INTERVAL 12 HOURS')),
    'left'
)

# 5. ç±»ä¼¼åœ°æ·»åŠ ç›®çš„åœ°æœºåœºå¤©æ°”
# ... (é‡å¤ä¸Šé¢çš„é€»è¾‘ï¼Œä½¿ç”¨ Dest å­—æ®µ)

# 6. ä¿å­˜ç»“æœ
flight_with_weather.write.parquet('flight_with_weather_features.parquet')

print("âœ“ å¤©æ°”æ•°æ®åˆå¹¶å®Œæˆï¼")
```

---

## ğŸ¯ é‡è¦çš„å¤©æ°”ç‰¹å¾

å¯¹èˆªç­å»¶è¯¯é¢„æµ‹æœ€é‡è¦çš„å­—æ®µï¼š

1. **temp** - æ¸©åº¦ï¼ˆÂ°Cï¼‰
   - æç«¯æ¸©åº¦å½±å“é£æœºæ€§èƒ½
   
2. **wspd** - é£é€Ÿï¼ˆkm/hï¼‰
   - å¼ºé£å¯¼è‡´æ— æ³•èµ·é™
   
3. **prcp** - é™æ°´é‡ï¼ˆmmï¼‰
   - é›¨é›ªå½±å“èƒ½è§åº¦å’Œè·‘é“æ¡ä»¶
   
4. **snow** - é™é›ªé‡ï¼ˆmmï¼‰
   - éœ€è¦é™¤å†°ï¼Œå¯¼è‡´ä¸¥é‡å»¶è¯¯
   
5. **wdir** - é£å‘ï¼ˆåº¦ï¼‰
   - ä¾§é£å½±å“èµ·é™

6. **pres** - æ°”å‹ï¼ˆhPaï¼‰
   - ä½æ°”å‹é€šå¸¸æ„å‘³ç€æ¶åŠ£å¤©æ°”

---

## ğŸ’¡ ç‰¹å¾å·¥ç¨‹å»ºè®®

```python
import pandas as pd
import numpy as np

def create_weather_features(df):
    """åˆ›å»ºæœ‰æ„ä¹‰çš„å¤©æ°”ç‰¹å¾"""
    
    # 1. æ¶åŠ£å¤©æ°”è¯„åˆ† (0-10)
    df['bad_weather_score'] = 0
    
    # ä½èƒ½è§åº¦ï¼ˆå‡è®¾ä½ æœ‰è¿™ä¸ªå­—æ®µï¼‰
    # df['bad_weather_score'] += np.where(df['visibility'] < 5, 3, 0)
    
    # å¼ºé£
    df['bad_weather_score'] += np.where(df['wspd'] > 40, 3, 
                                 np.where(df['wspd'] > 25, 2, 0))
    
    # é™æ°´
    df['bad_weather_score'] += np.where(df['prcp'] > 10, 3,
                                 np.where(df['prcp'] > 2, 2, 0))
    
    # é™é›ª
    df['bad_weather_score'] += np.where(df['snow'] > 5, 4,
                                 np.where(df['snow'] > 0, 2, 0))
    
    # æç«¯æ¸©åº¦
    df['bad_weather_score'] += np.where((df['temp'] < -10) | (df['temp'] > 38), 1, 0)
    
    # 2. å¤©æ°”å˜åŒ–ç‡
    df = df.sort_values(['airport_code', 'time'])
    df['temp_change_3h'] = df.groupby('airport_code')['temp'].diff(3)
    df['wind_change_3h'] = df.groupby('airport_code')['wspd'].diff(3)
    
    # 3. æ˜¯å¦æœ‰é™æ°´
    df['has_precipitation'] = (df['prcp'] > 0).astype(int)
    df['has_snow'] = (df['snow'] > 0).astype(int)
    
    # 4. é£å¯’æŒ‡æ•°ï¼ˆå†¬å­£ï¼‰
    df['wind_chill'] = 13.12 + 0.6215*df['temp'] - \
                       11.37*(df['wspd']**0.16) + \
                       0.3965*df['temp']*(df['wspd']**0.16)
    
    return df

# ä½¿ç”¨
weather_df = pd.read_parquet('weather_data_2018_2022.parquet')
weather_with_features = create_weather_features(weather_df)
weather_with_features.to_parquet('weather_with_features.parquet')
```

---

## ğŸš¨ å¸¸è§é—®é¢˜è§£å†³

### é—®é¢˜1ï¼šæŸäº›æœºåœºæ‰¾ä¸åˆ°æ°”è±¡ç«™
**è§£å†³**ï¼šä½¿ç”¨é™„è¿‘å¤§å‹æœºåœºçš„å¤©æ°”æ•°æ®
```python
# ä¸ºå°æœºåœºæŒ‡å®šä½¿ç”¨é‚»è¿‘å¤§æœºåœºçš„æ°”è±¡ç«™
small_airport_mapping = {
    'ISP': 'JFK',  # Long Islandä½¿ç”¨JFKçš„å¤©æ°”
    'HPN': 'LGA',  # White Plainsä½¿ç”¨LaGuardiaçš„å¤©æ°”
    # ...
}
```

### é—®é¢˜2ï¼šæ•°æ®æœ‰ç¼ºå¤±å€¼
**è§£å†³**ï¼šä½¿ç”¨æ’å€¼æˆ–å†å²å‡å€¼
```python
# å‰åæ’å€¼
df['temp'] = df.groupby('airport_code')['temp'].transform(
    lambda x: x.interpolate(method='linear')
)

# æˆ–ä½¿ç”¨åŒæœŸå†å²å‡å€¼
df['temp'] = df.groupby(['airport_code', df['time'].dt.month, df['time'].dt.hour])['temp'].transform(
    lambda x: x.fillna(x.mean())
)
```

### é—®é¢˜3ï¼šä¸‹è½½å¤ªæ…¢
**è§£å†³**ï¼šä½¿ç”¨å¤šçº¿ç¨‹æˆ–ç›´æ¥ä¸‹è½½NOAA ISDæ–‡ä»¶
```python
from concurrent.futures import ThreadPoolExecutor

def download_year(args):
    airport_code, station_id, year = args
    # ... ä¸‹è½½é€»è¾‘
    
with ThreadPoolExecutor(max_workers=5) as executor:
    tasks = [(code, station, year) 
             for code, station in zip(codes, stations)
             for year in range(2018, 2023)]
    executor.map(download_year, tasks)
```

---

## ğŸ“Š é¢„æœŸæ•°æ®è§„æ¨¡

- **100ä¸ªæœºåœº Ã— 5å¹´**
- æ¯ä¸ªæœºåœº: ~43,800æ¡è®°å½• (365å¤© Ã— 24å°æ—¶ Ã— 5å¹´)
- æ€»è®°å½•æ•°: ~4,380,000æ¡
- å­˜å‚¨ç©ºé—´: 
  - CSVæ ¼å¼: ~2-3 GB
  - Parquetæ ¼å¼: ~500 MB - 1 GBï¼ˆæ¨èï¼‰

---

## âœ… æ£€æŸ¥æ¸…å•

åœ¨å¼€å§‹ä½ çš„é¡¹ç›®ä¹‹å‰ï¼Œç¡®ä¿ï¼š

- [ ] meteostatåº“å®‰è£…æˆåŠŸ
- [ ] æµ‹è¯•è„šæœ¬è¿è¡ŒæˆåŠŸ
- [ ] æœºåœºåæ ‡åˆ—è¡¨å‡†å¤‡å®Œæˆ
- [ ] äº†è§£å®Œæ•´å·¥ä½œæµç¨‹
- [ ] çŸ¥é“å¦‚ä½•å¤„ç†ç¼ºå¤±æ•°æ®
- [ ] è®¡åˆ’å¥½ç‰¹å¾å·¥ç¨‹ç­–ç•¥
- [ ] äº†è§£å¦‚ä½•ä¸PySparké›†æˆ

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

å‚è€ƒä»¥ä¸‹æ–‡ä»¶ï¼š
1. `noaa_weather_data_guide.py` - è¯¦ç»†æŠ€æœ¯æ–‡æ¡£
2. `test_weather_download.py` - æµ‹è¯•è„šæœ¬
3. `create_airport_mapping.py` - æœºåœºæ˜ å°„å·¥å…·

ç¥é¡¹ç›®é¡ºåˆ©ï¼ğŸš€
